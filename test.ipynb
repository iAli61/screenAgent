{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e511724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ea3053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Endpoint: https://aoai-sweden-505.openai.azure.com/\n",
      "Azure API Key available: Yes\n",
      "Azure API Version: 2025-03-01-preview\n",
      "Model deployment name: gpt-4o\n",
      "Endpoint URL: https://aoai-sweden-505.openai.azure.com//openai/deployments/gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI configuration\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_api_version = os.getenv(\"OPENAI_API_VERSION\", \"2023-07-01-preview\")\n",
    "model = os.getenv(\"LLM_MODEL\", \"gpt-35-turbo\")\n",
    "\n",
    "print(f\"Azure Endpoint: {azure_endpoint}\")\n",
    "print(f\"Azure API Key available: {'Yes' if azure_api_key else 'No'}\")\n",
    "print(f\"Azure API Version: {azure_api_version}\")\n",
    "print(f\"Model deployment name: {model}\")\n",
    "\n",
    "# Initialize Azure OpenAI client if enabled\n",
    "\n",
    "# Create endpoint URL with deployment name\n",
    "endpoint_url = f\"{azure_endpoint}/openai/deployments/{model}\"\n",
    "print(f\"Endpoint URL: {endpoint_url}\")\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint_url,\n",
    "    credential=AzureKeyCredential(azure_api_key),\n",
    "    api_version=azure_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129d4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The Eiffel Tower is a marvel of engineering and design, and it holds a special place in both the history and the cultural identity of Paris. Here are some reasons why the Eiffel Tower is considered so remarkable:\n",
      "\n",
      "1. **Architectural Innovation**: Designed by Gustave Eiffel and completed in 1889 for the Exposition Universelle (World's Fair), the tower was an engineering wonder of its time. Made from iron lattice, its construction demonstrated groundbreaking techniques that have influenced the field of civil engineering.\n",
      "\n",
      "2. **Iconic Status**: The Eiffel Tower is an emblem of Paris and a symbol of France around the world. Its unique and elegant design has made it one of the most recognizable structures globally.\n",
      "\n",
      "3. **Stunning Views**: Visitors can ascend the tower to enjoy panoramic views of Paris from the observation decks. The vista stretches across the city, offering sights of famous landmarks such as the Seine River, Sacr√©-C≈ìur, and Notre-Dame Cathedral.\n",
      "\n",
      "4. **Cultural Significance**: Over the years, the Eiffel Tower has been featured in countless films, books, and artworks, further cementing its status as a cultural icon. Its presence adds to the romantic allure of Paris.\n",
      "\n",
      "5. **Artistic Illuminations**: At night, the Eiffel Tower is illuminated by thousands of lights, creating a mesmerizing and magical effect that draws both locals and tourists alike. Special light shows and events are often held, adding to its appeal.\n",
      "\n",
      "6. **Historical Significance**: Initially met with skepticism, the Eiffel Tower proved to be a durable and lasting symbol of technological progress and has stood the test of time, attracting millions of visitors each year.\n",
      "\n",
      "The Eiffel Tower's combination of historical importance, architectural beauty, and cultural symbolism makes it a must-see destination for anyone visiting Paris.\n",
      "Model: gpt-4o-2024-08-06\n",
      "Usage:\n",
      "\tPrompt tokens: 202\n",
      "\tTotal tokens: 562\n",
      "\tCompletion tokens: 360\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I am going to Paris, what should I see?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is so great about #1?\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 2048\n",
    "}\n",
    "response = client.complete(payload)\n",
    "\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\tCompletion tokens:\", response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e292c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Endpoint: https://aoai-sweden-505.openai.azure.com/\n",
      "Azure API Key available: Yes\n",
      "Azure API Version: 2025-03-01-preview\n",
      "Model deployment name: gpt-4o\n",
      "Endpoint URL: https://aoai-sweden-505.openai.azure.com//openai/deployments/gpt-4o\n",
      "Response: The image appears to be a screenshot of a web page or user interface component. It includes text that says \"Smart screen monitoring with\" at the top, indicating it might be part of a larger context or heading. Below this text, there are two tabs labeled \"Screenshots\" and \"Monitor\", with \"Screenshots\" highlighted in blue, suggesting it is currently selected and active. This could be part of a software tool or system for screen monitoring.\n",
      "Model: gpt-4o-2024-08-06\n",
      "Usage:\n",
      "\tPrompt tokens: 287\n",
      "\tTotal tokens: 378\n",
      "\tCompletion tokens: 91\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_api_version = os.getenv(\"OPENAI_API_VERSION\", \"2023-07-01-preview\")\n",
    "model = os.getenv(\"LLM_MODEL\", \"gpt-35-turbo\")\n",
    "\n",
    "print(f\"Azure Endpoint: {azure_endpoint}\")\n",
    "print(f\"Azure API Key available: {'Yes' if azure_api_key else 'No'}\")\n",
    "print(f\"Azure API Version: {azure_api_version}\")\n",
    "print(f\"Model deployment name: {model}\")\n",
    "\n",
    "# Initialize Azure OpenAI client if enabled\n",
    "\n",
    "# Create endpoint URL with deployment name\n",
    "endpoint_url = f\"{azure_endpoint}/openai/deployments/{model}\"\n",
    "print(f\"Endpoint URL: {endpoint_url}\")\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint_url,\n",
    "    credential=AzureKeyCredential(azure_api_key),\n",
    "    api_version=azure_api_version\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_image(image_path: str) -> Optional[Image.Image]:\n",
    "    \"\"\"\n",
    "    Validate if the image is suitable for processing.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if image is valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode not in ('RGB', 'L'):\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            max_dimension = 2048\n",
    "            if img.width > max_dimension or img.height > max_dimension:\n",
    "                ratio = min(max_dimension / img.width, max_dimension / img.height)\n",
    "                new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "                img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            img_byte_arr = io.BytesIO()\n",
    "            img.save(img_byte_arr, format='PNG', quality=85)\n",
    "            img_byte_arr.seek(0)\n",
    "            \n",
    "            return Image.open(img_byte_arr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Image preprocessing failed for {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def encode_image(image_path: str) -> Optional[str]:\n",
    "    \"\"\"Encode image as base64 with proper validation and preprocessing.\"\"\"\n",
    "    try:\n",
    "        \n",
    "\n",
    "        processed_img = preprocess_image(image_path)\n",
    "        if processed_img is None:\n",
    "            return None\n",
    "\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        processed_img.save(img_byte_arr, format='PNG', quality=85)\n",
    "        img_byte_arr.seek(0)\n",
    "        base64_encoded = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
    "\n",
    "        try:\n",
    "            base64.b64decode(base64_encoded)\n",
    "            return base64_encoded\n",
    "        except Exception as e:\n",
    "            print(f\"Base64 validation failed for {image_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Image encoding failed for {image_path}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "prompt = \"What is in this image?\"\n",
    "\n",
    "# Image file path\n",
    "IMAGE_PATH = 'screenshots/run_20250624_102302_c0ee4d92/images/region_91b32eaa-f938-499e-985b-7f73f40e4143.png'\n",
    "\n",
    "\n",
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant that can analyze images and provide information about them.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2048\n",
    "}\n",
    "response = client.complete(payload)\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\tCompletion tokens:\", response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45763c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Image file path\u001b[39;00m\n\u001b[32m      4\u001b[39m IMAGE_PATH = \u001b[33m'\u001b[39m\u001b[33mscreenshots/run_20250624_102302_c0ee4d92/images/region_91b32eaa-f938-499e-985b-7f73f40e4143.png\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m base64_image = \u001b[43mencode_image\u001b[49m(IMAGE_PATH)\n\u001b[32m      9\u001b[39m payload = {\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     11\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2048\u001b[39m\n\u001b[32m     32\u001b[39m }\n\u001b[32m     33\u001b[39m response = client.complete(payload)\n",
      "\u001b[31mNameError\u001b[39m: name 'encode_image' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"What is in this image?\"\n",
    "\n",
    "# Image file path\n",
    "IMAGE_PATH = 'screenshots/run_20250624_102302_c0ee4d92/images/region_91b32eaa-f938-499e-985b-7f73f40e4143.png'\n",
    "\n",
    "\n",
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant that can analyze images and provide information about them.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2048\n",
    "}\n",
    "response = client.complete(payload)\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\tCompletion tokens:\", response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5130f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint URL: https://aoai-sweden-505.openai.azure.com//openai/deployments/gpt-4o\n",
      "Response: The image appears to be a cropped portion of a webpage or a digital document. It features text that reads \"Smart screen monitoring with\" at the top. Below that, there are two tabs: \"Screenshots\" and \"Monitor,\" with \"Screenshots\" highlighted or selected, indicated by blue text and a blue underline. The layout suggests a navigation bar or menu where users can switch between different sections or options related to screen monitoring.\n",
      "Model: gpt-4o-2024-08-06\n",
      "Usage:\n",
      "\tPrompt tokens: 286\n",
      "\tTotal tokens: 373\n",
      "\tCompletion tokens: 87\n",
      "Response: The image shows a cropped portion of a webpage or software interface. At the top is partial text reading ‚ÄúSmart screen monitoring wit‚Ä¶‚Äù (the remainder is cut off). Below that, on a white background, there appears to be a horizontal navigation or tab bar. One tab is labeled ‚ÄúScreenshots‚Äù in blue text and is currently active, indicated by a solid blue underline stretching across the tab. To the right of it is another, inactive tab whose visible text begins with ‚ÄúMonitor‚Ä¶‚Äù.\n",
      "Model: o3-2025-04-16\n",
      "Usage:\n",
      "\tPrompt tokens: 240\n",
      "\tTotal tokens: 485\n",
      "\tCompletion tokens: 245\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage, TextContentItem, ImageContentItem, ImageUrl, ImageDetailLevel\n",
    "\n",
    "\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_api_version = os.getenv(\"OPENAI_API_VERSION\", \"2023-07-01-preview\")\n",
    "model = os.getenv(\"LLM_MODEL\", \"gpt-35-turbo\")\n",
    "\n",
    "# print(f\"Azure Endpoint: {azure_endpoint}\")\n",
    "# print(f\"Azure API Key available: {'Yes' if azure_api_key else 'No'}\")\n",
    "# print(f\"Azure API Version: {azure_api_version}\")\n",
    "# print(f\"Model deployment name: {model}\")\n",
    "\n",
    "models = [model, \"o3\"]\n",
    "\n",
    "# Initialize Azure OpenAI client if enabled\n",
    "\n",
    "# Create endpoint URL with deployment name\n",
    "endpoint_url = f\"{azure_endpoint}/openai/deployments/{model}\"\n",
    "print(f\"Endpoint URL: {endpoint_url}\")\n",
    "aoai_client = ChatCompletionsClient(\n",
    "    endpoint=endpoint_url,\n",
    "    credential=AzureKeyCredential(azure_api_key),\n",
    "    api_version=azure_api_version\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "github_token = os.getenv(\"GITHUB_MODEL_TOKEN\")\n",
    "github_client = ChatCompletionsClient(\n",
    "    endpoint=\"https://models.inference.ai.azure.com\",\n",
    "    credential=AzureKeyCredential(github_token),\n",
    "    api_version=azure_api_version\n",
    ")\n",
    "\n",
    "img_path = \"screenshots/run_20250624_102302_c0ee4d92/images/region_91b32eaa-f938-499e-985b-7f73f40e4143.png\"\n",
    "base64_image = encode_image(img_path)\n",
    "\n",
    "# Create message with image content\n",
    "messages = [\n",
    "    SystemMessage(\"You are an AI assistant that analyzes screenshots and describes them in detail.\"),\n",
    "    UserMessage([\n",
    "        TextContentItem(text=prompt),\n",
    "        ImageContentItem(\n",
    "            image_url=ImageUrl(\n",
    "                url=f\"data:image/png;base64,{base64_image}\",\n",
    "                detail=ImageDetailLevel.HIGH,\n",
    "            ),\n",
    "        ),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "for client, model in zip([aoai_client, github_client], models):\n",
    "    try:\n",
    "        response = client.complete(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "        )\n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "        print(\"Model:\", response.model)\n",
    "        print(\"Usage:\")\n",
    "        print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "        print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "        print(\"\tCompletion tokens:\", response.usage.completion_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with {client}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df88367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The image is a small snippet of a web-app interface. Across the top you can see the title text (cut off) reading ‚ÄúSmart screen monitoring wit‚Ä¶‚Äù and just below it is a horizontal tab bar. The ‚ÄúScreenshots‚Äù tab is highlighted in blue (indicating it‚Äôs active) and immediately to its right is an unselected ‚ÄúMonitoring‚Äù tab.\n",
      "Model: o4-mini-2025-04-16\n",
      "Usage:\n",
      "\tPrompt tokens: 101\n",
      "\tTotal tokens: 386\n",
      "\tCompletion tokens: 285\n"
     ]
    }
   ],
   "source": [
    "model = \"o4-mini\"\n",
    "response = github_client.complete(messages=messages,\n",
    "                           model=model,\n",
    "                           )\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\tCompletion tokens:\", response.usage.completion_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c52e6e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is Madrid.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_MODEL_TOKEN\"]\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model_name = \"openai/o3\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": \"You are a helpful assistant.\",\n",
    "    },\n",
    "    UserMessage(\"What is the capital of France?\"),\n",
    "    AssistantMessage(\"The capital of France is Paris.\"),\n",
    "    UserMessage(\"What about Spain?\"),\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bd36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f5db9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing refactored LLM API...\n",
      "ü§ñ Azure AI client initialized - Endpoint: https://aoai-sweden-505.openai.azure.com//openai/deployments/gpt-4o\n",
      "ü§ñ GitHub Models client initialized\n",
      "OpenAI setup failed: OpenAI API key not found\n",
      "Available providers: ['azure', 'github']\n",
      "Is available: True\n",
      "\n",
      "Testing analysis with refactored API...\n",
      "‚úÖ Analysis successful: The image shows a portion of a webpage or application interface. At the top, there is a partial text that reads \"Smart screen monitoring wit,\" suggesting a heading or title related to screen monitoring. Below this, there are two tabs labeled \"Screenshots\" and \"Monitor.\" The \"Screenshots\" tab is highlighted in blue, indicating that it is currently selected or active. The interface has a clean and minimal design, with a white background and simple text formatting.\n"
     ]
    }
   ],
   "source": [
    "# Test the refactored LLM API\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/alibina/repo/screenAgent')\n",
    "\n",
    "from src.api.llm_api import LLMAnalyzer\n",
    "\n",
    "# Create analyzer with LLM enabled\n",
    "config = {\n",
    "    'llm_enabled': True,\n",
    "    'llm_prompt': 'What is in this image?',\n",
    "    'llm_model': os.getenv('LLM_MODEL', 'gpt-4o')\n",
    "}\n",
    "\n",
    "print(\"Testing refactored LLM API...\")\n",
    "analyzer = LLMAnalyzer(config)\n",
    "print(f\"Available providers: {analyzer.get_available_providers()}\")\n",
    "print(f\"Is available: {analyzer.is_available()}\")\n",
    "\n",
    "# Test with the same image\n",
    "image_path = \"screenshots/run_20250624_102302_c0ee4d92/images/region_91b32eaa-f938-499e-985b-7f73f40e4143.png\"\n",
    "\n",
    "if analyzer.is_available():\n",
    "    print(\"\\n=== Testing Azure provider ===\")\n",
    "    result_azure = analyzer.analyze_image_from_path(image_path, \"What is in this image?\", provider='azure')\n",
    "    if result_azure:\n",
    "        print(f\"‚úÖ Azure analysis successful: {result_azure[:100]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå Azure analysis failed\")\n",
    "    \n",
    "    print(\"\\n=== Testing GitHub provider ===\")\n",
    "    result_github = analyzer.analyze_image_from_path(image_path, \"What is in this image?\", provider='github')\n",
    "    if result_github:\n",
    "        print(f\"‚úÖ GitHub analysis successful: {result_github[:100]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå GitHub analysis failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No providers available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
